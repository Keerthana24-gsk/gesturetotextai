Introduction:
In a world increasingly driven by human-computer interaction, hand gestures offer a natural and intuitive way to communicate, especially for individuals with speech or hearing impairments. This project introduces a gesture-to-text conversion system that uses a standard webcam and machine learning to recognize static hand gestures and translate them into meaningful text. By leveraging Google’s MediaPipe for real-time hand tracking and a Random Forest classifier for gesture recognition, the system operates efficiently without the need for expensive sensors or wearable devices. It aims to provide an accessible, low-cost solution for inclusive communication and assistive technology applications.
Abstract:
This project presents a real-time hand gesture-to-text conversion system using computer vision and machine learning. The system employs a standard webcam to capture static hand gestures, which are processed using Google’s MediaPipe to extract 21 hand landmarks. These landmarks form a feature vector that is classified using a trained Random Forest model. Each gesture is mapped to a predefined textual label such as "Hello", "Eat", or "Drink". The solution is lightweight and does not require additional hardware like gloves or sensors, making it affordable and accessible. The model achieves satisfactory accuracy when trained on gesture-specific landmark data and operates efficiently on standard laptops. This system has potential applications in assistive technologies for the speech- and hearing-impaired and in gesture-based interfaces for human-computer interaction
Conclusion:
The developed system successfully converts static hand gestures into text using computer vision and machine learning, demonstrating its potential as a low-cost, real-time assistive tool. The combination of MediaPipe's landmark detection and a trained classifier enables accurate gesture recognition on standard hardware. This project highlights how AI can enhance accessibility for individuals with speech and hearing disabilities, and it lays the groundwork for future advancements such as dynamic gesture recognition, speech output integration, and multilingual support in gesture-based communication systems.
